# distill_student.yaml
agent:
  algorithm:
    class_name: Distillation
    num_learning_epochs: 5
    learning_rate: 1e-3
    gradient_length: 32
    max_grad_norm: 1.0
  policy:
    class_name: StudentTeacher
    init_noise_std: 0.1
    student_hidden_dims: [256, 256, 256]
    teacher_hidden_dims: [256, 256, 256]   # match your PPO actor
    activation: elu
  num_steps_per_env: 32
  max_iterations: 2000
  empirical_normalization: true
  save_interval: 50
